{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f40bf0-52c1-4d3a-ac34-419ea041f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from seaborn) (3.10.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\unger\\onedrive\\dokumente\\sebsdocu2025\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas seaborn\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "data_dir = Path(r\"C:\\Users\\alexa\\Desktop\\Untitled Folder\")  # Pfad ggf. anpassen\n",
    "files = sorted(data_dir.glob(\"*.json\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb02390-de7e-4f33-acf7-fb0e3ce90404",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         gc\u001b[38;5;241m.\u001b[39mcollect()  \u001b[38;5;66;03m# Speicher freigeben\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Endgültig alle Batches zusammenfügen\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\unger\\OneDrive\\Dokumente\\SebsDocu2025\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\unger\\OneDrive\\Dokumente\\SebsDocu2025\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\unger\\OneDrive\\Dokumente\\SebsDocu2025\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "\n",
    "batch_size = 20  # Anzahl Dateien pro Batch, kann je nach RAM angepasst werden\n",
    "dfs_batch = []\n",
    "dfs_final = []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    try:\n",
    "        dfs_batch.append(pd.read_json(f, lines=True))\n",
    "    except ValueError:\n",
    "        obj = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(obj, list):\n",
    "            dfs_batch.append(pd.DataFrame(obj))\n",
    "        elif isinstance(obj, dict):\n",
    "            dfs_batch.append(pd.json_normalize(obj))\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekanntes JSON-Format in {f.name}\")\n",
    "\n",
    "    # Sobald batch_size erreicht ist oder letzte Datei, zusammenfügen und aufsammeln\n",
    "    if (i + 1) % batch_size == 0 or (i + 1) == len(files):\n",
    "        temp_df = pd.concat(dfs_batch, ignore_index=True)\n",
    "        dfs_final.append(temp_df)\n",
    "        dfs_batch.clear()\n",
    "        gc.collect()  # Speicher freigeben\n",
    "\n",
    "# Endgültig alle Batches zusammenfügen\n",
    "df = pd.concat(dfs_final, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce54ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721a45ca-6b40-4ac1-a8b5-20ae48e49daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Beispielhafte Umbenennung auf erwartete Spalten\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rename_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_time_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec_time_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer_mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_transfer_mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{k:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m rename_map\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns})\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Metadaten aus Dateinamen extrahieren (optional, je nach Namensschema)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_meta\u001b[39m(path: Path):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Beispiel: aws_eu_110_cold_results_256.json\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Beispielhafte Umbenennung auf erwartete Spalten\n",
    "rename_map = {\n",
    "    \"duration_ms\": \"exec_time_ms\",\n",
    "    \"duration\": \"exec_time_ms\",\n",
    "    \"latency\": \"latency_ms\",\n",
    "    \"cpu_time_ms\": \"cpu_ms\",\n",
    "    \"memory_gb_s\": \"mem_mb_s\",  # ggf. skaliert konvertieren\n",
    "    \"transfer_mb\": \"data_transfer_mb\"\n",
    "}\n",
    "df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
    "\n",
    "# Metadaten aus Dateinamen extrahieren (optional, je nach Namensschema)\n",
    "def extract_meta(path: Path):\n",
    "    # Beispiel: aws_eu_110_cold_results_256.json\n",
    "    parts = path.stem.split(\"_\")\n",
    "    meta = {}\n",
    "    if len(parts) >= 6:\n",
    "        meta[\"provider\"] = parts\n",
    "        meta[\"scenario\"] = parts[12]\n",
    "        meta[\"load\"] = int(parts[13]) if parts[13].isdigit() else parts[13]\n",
    "        meta[\"start_type\"] = parts[14]\n",
    "        meta[\"platform\"] = \"lambda\"  # Beispiel; ggf. aus Inhalt ableiten\n",
    "        meta[\"settings\"] = {\"memory\": parts[15]}\n",
    "    return meta\n",
    "\n",
    "meta_rows = []\n",
    "for f in files:\n",
    "    meta_rows.append(extract_meta(f))\n",
    "import pandas as pd\n",
    "meta_df = pd.DataFrame(meta_rows)\n",
    "df[[c for c in meta_df.columns if c not in df.columns]] = meta_df[[c for c in meta_df.columns if c not in df.columns]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f98d89-4c30-40f8-bc76-60dfd382a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, json\n",
    "\n",
    "data_dir = Path(r\"C:\\Users\\alexa\\Desktop\\Untitled Folder\")\n",
    "print(\"Suche in:\", data_dir.resolve())\n",
    "\n",
    "# 1) Dateien sammeln: .json und .jsonl, notfalls rekursiv\n",
    "files = sorted(list(data_dir.glob(\"*.json\"))) + sorted(list(data_dir.glob(\"*.jsonl\")))\n",
    "if not files:\n",
    "    files = sorted(list(data_dir.rglob(\"*.json\"))) + sorted(list(data_dir.rglob(\"*.jsonl\")))\n",
    "print(f\"{len(files)} Dateien gefunden\")\n",
    "print([f.name for f in files[:5]])  # Stichprobe\n",
    "\n",
    "def load_one(f: Path) -> pd.DataFrame:\n",
    "    # Versuch A: JSON-Lines\n",
    "    try:\n",
    "        dfA = pd.read_json(f, lines=True)\n",
    "        if not dfA.empty:\n",
    "            return dfA\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Versuch B: Normales JSON\n",
    "    obj = json.loads(f.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(obj, list):\n",
    "        return pd.DataFrame(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        # Heuristik: erst Liste von Records in einem Key suchen\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(v, list) and v and isinstance(v, dict):\n",
    "                return pd.json_normalize(v)\n",
    "        # Sonst Top-Level-Dict flatten\n",
    "        return pd.json_normalize(obj)\n",
    "    # Fallback: leere Tabelle\n",
    "    return pd.DataFrame()\n",
    "\n",
    "parts = []\n",
    "for f in files:\n",
    "    dfi = load_one(f)\n",
    "    print(f\"{f.name}: {dfi.shape}\")\n",
    "    parts.append(dfi)\n",
    "\n",
    "df = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "print(\"Gesamt:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68c2ba-a5dd-4591-abc3-5e3bb2c382ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = {\"provider\",\"platform\",\"scenario\",\"load\",\"start_type\",\"exec_time_ms\",\"latency_ms\",\n",
    "            \"cpu_ms\",\"mem_mb_s\",\"data_transfer_mb\",\"cost\",\"settings\"}\n",
    "missing = expected - set(df.columns)\n",
    "if missing:\n",
    "    # Beispiel: fehlende Spalten mit NaN auffüllen (besser: korrekt mappen/ableiten)\n",
    "    for col in missing:\n",
    "        df[col] = pd.NA\n",
    "# harte Prüfung\n",
    "missing = expected - set(df.columns)\n",
    "assert not missing, f\"Fehlende Spalten: {missing}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c758d-3e48-4bba-b7f5-0ee5ee10dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"exec_time_ms\",\"latency_ms\",\"cost\"]\n",
    "\n",
    "def iqr_filter(group, cols):\n",
    "    for c in cols:\n",
    "        q1 = group[c].quantile(0.25)\n",
    "        q3 = group[c].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5*iqr\n",
    "        upper = q3 + 1.5*iqr\n",
    "        group = group[(group[c] >= lower) & (group[c] <= upper)]\n",
    "    return group\n",
    "\n",
    "df_clean = (df\n",
    "    .groupby([\"provider\",\"platform\",\"scenario\",\"load\",\"start_type\"], group_keys=False)\n",
    "    .apply(lambda g: iqr_filter(g, metrics)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d770ded-7bca-4ec4-b9c5-6d07d8ca41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(\n",
    "    data=df_clean, kind=\"box\",\n",
    "    x=\"provider\", y=\"latency_ms\",\n",
    "    col=\"load\", row=\"start_type\",\n",
    "    sharey=False, height=3, aspect=1.2\n",
    ")\n",
    "g.set_xticklabels(rotation=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bd460-c5c9-4f53-9331-dea75581b1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122bd73-3687-4ce6-b801-010a3af60214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
