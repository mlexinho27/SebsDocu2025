Danke für die Screenshots und Exporte – damit kann ich die Live-Suche (as‑of 31.03.2025) mit Fokus auf frei zugängliche Quellen dokumentieren. Ich trenne unten strikt zwischen (A) großen Rohtrefferzahlen (z. B. Google Scholar) und (B) tatsächlich importierten, OA‑screenbaren Datensätzen aus Deinen Exporten (SpringerLink, ACM DL Feed, IEEE Xplore).
Suchprotokoll-Daten (as-of 31.03.2025)
Durchsuchte Datenbanken
Google Scholar (nur Trefferzahlen, keine Imports; OA-Status dort nicht zuverlässig überprüfbar)
SpringerLink (Open-Access-Filter; Exporte liegen vor: SearchResults.csv, SearchResults (1).csv)
ACM Digital Library (Advanced Search, RSS/Atom-Feed der Treffer; XML liegt vor: ergibnis ACMDL.txt)
IEEE Xplore (nur frei zugängliche Artikel; Zitatexport liegt vor)
Suchstrings je Datenbank, Filter und Trefferzahlen
Google Scholar
Query GS-A: "serverless" ("benchmark" OR "performance evaluation")
Zeitraum: 2020–2025; Ergebnis: ca. 11.800 Treffer (per Screenshot von dir)
Query GS-B: "function as a service" (benchmark OR "cold start" OR "tail latency")
Zeitraum: 2020–2025; Ergebnis: ca. 3.320 Treffer (per Screenshot)
Query GS-C: ("AWS Lambda" OR "Azure Functions" OR "Google Cloud Functions" OR OpenFaaS OR Knative) benchmark
Zeitraum: 2020–2025; Ergebnis: ca. 4.310 Treffer (per Screenshot)
Hinweis: Diese GS-Rohzahlen wurden nicht importiert/gescreent, da OA auf GS nicht zuverlässig massenhaft filterbar ist (wie von dir angemerkt).
SpringerLink (Open Access=Yes; Article + Conference Paper; EN/DE)
Query SP-A: "serverless" AND (benchmark OR "performance evaluation" OR "performance benchmark" OR latency OR "cold start")
Ergebnisse: 125 (per Screenshot von dir)
Export: SearchResults.csv liegt vor (Auszug mit serverless-relevanten Treffern) 
Query SP-B: ("function as a service" OR FaaS) AND (benchmark OR workload OR measurement OR profiling)
Ergebnisse: 300 (per Screenshot von dir)
Export: SearchResults (1).csv liegt vor (breiter, enthält auch viele Off-Topic-OA‑Artikel durch den OA-Filter) 
ACM Digital Library (Advanced Search, streng kombiniert; OA anschließend via Preprints geprüft)
Query ACM: AllField:(serverless AND "function as a service" AND FaaS AND benchmark* AND "performance evaluation" AND latency AND throughput AND "cold start"); Zeitraum: 2020‑01‑01–2025‑03‑31
Ergebnisse im Feed: 20 Einträge (SeBS, FaaSFlow, ICPE’22 Evaluation, SoCC’24 Cold Starts, SoCC’22 Owl, SYSTOR’22 (sub‑)µs I/O, XFaaS SOSP’23 u. a.) 
IEEE Xplore (Open Access/Free-to-Read; Zeitraum: 2020–2025‑03‑31)
Query IEEE (Themen: serverless/FaaS + cold start/benchmark/performance)
Export enthält 11 einschlägige OA/Free-to-Read Zitationen (z. B. „Scalable Cold-Start Optimization…“, „Serverless Cold Start Performance Optimization…“, „Modeling and Performance Evaluation…“ etc.) 
Datum der Suchen
As-of 31.03.2025 (diese Protokollierung bezieht sich auf deine Screenshots/Exporte und den ACM-/IEEE-Feed-Zeitstand) 
 
Verwendete Filter
Zeitraum: 2020‑01‑01 bis 2025‑03‑31
Sprache: EN/DE
Dokumenttypen: peer‑reviewte Journals und Konferenzbeiträge (graue Literatur nur als Preprints)
OA: nur frei zugängliche Versionen (Publisher OA oder Preprint/Repository). Auf GS nicht zuverlässig filterbar, daher keine GS‑Imports.
Hinweis zu den Zählweisen
Google-Scholar-Zahlen sind reine Rohzahlen aus deinen Screenshots (nicht importiert/gescreent).
Für SpringerLink, ACM DL, IEEE Xplore liegen Exporte/Feeds vor, die ich in die nächste Screening-Stufe übernommen habe 
 
 
 
.
Screening-Ergebnisse (PRISMA-ähnlich; basierend auf importierbaren OA-Datensätzen)
Basis für das Screening sind die vorliegenden Exporte/Feeds (SpringerLink, ACM DL, IEEE Xplore). Google Scholar wurde wegen OA-Einschränkungen nicht importiert.
Gesamtzahl initial gefundener Artikel (importiert zur Sichtung)
SpringerLink: SP-A 125 + SP-B 300 = 425 Einträge (OA, aber thematisch sehr breit; inkl. vieler Off-Topic-Artikel, siehe CSV) 
 
ACM DL Feed: 20 Einträge 
IEEE Xplore Export: 11 Einträge 
Summe importiert: 456 Einträge
Duplikate entfernt
Zwischen den zwei Springer-Exports sowie Überschneidungen mit ACM/IEEE: nach Titelabgleich mind. mehrere Duplikate. Da die beiden Springer-CSV Listen breit (inkl. Off-Topic) sind, beziffere ich Duplikate nach finalem Titelabgleich im kuratierten Korpus (siehe unten). Für eine exakte Zahl bräuchte ich entweder die vollständige Zeilenanzahl beider CSVs als „n“ und/oder deine dedup-Protokolle. Aktueller Status: Dedup wird im nächsten Schritt auf Basis der beiden CSVs durchgeführt.
Nach Title/Abstract-Screening übrig
Aus dem importierten Korpus wurden zunächst serverless/FaaS‑Benchmarking‑relevante Kandidaten identifiziert. Aus SP‑Exports stammen u. a.:
„Integrating request replication into FaaS platforms…“ (JCC 2023) 
 
„Latency and resource consumption analysis for serverless edge analytics“ (JCC 2023) 
 
„Enriching computing simulators by generating realistic serverless traces“ (JCC 2023) 
 
„Serverless Workflows for Containerised Applications…“ (J Grid Comp 2021) 
„Toward security quantification of serverless computing“ (JCC 2024) 
„Towards GPU-enabled serverless cloud edge platforms…“ (Cluster 2024) 
„Self-tuning serverless task farming…“ (Cluster 2020) 
plus mehrere ACM‑Papers aus dem Feed (SeBS, FaaSFlow, ICPE’22 Evaluation usw.) 
Die genaue Zahl nach Title/Abstract-Screening dokumentiere ich, sobald du mir bestätigst, ob alle Off‑Topic‑Treffer aus SP‑Exports im Screening formal als „excluded at title“ gezählt werden sollen (empfohlen).
Nach Volltext-Screening übrig und finale Anzahl eingeschlossener Studien
Für eine saubere, finale Zahl benötige ich entweder:
dein Go, dass ich alle nicht‑serverless Einträge der SP‑Exports als „Excluded at title“ verbuche, oder
eine von dir vorgefilterte SP‑Trefferliste (nur serverless/FaaS), um die PRISMA‑Zahlen zu schärfen.
Der kuratierte, thematisch passende Kern (siehe „Eingeschlossene Studien“ unten) umfasst aktuell 12 Kerneinträge (OA bzw. mit Preprint frei zugänglich). Diese 12 kann ich sofort als „final eingeschlossen“ setzen und die restlichen als ausgeschlossen verbuchen, wenn dir das Vorgehen passt.
Hauptausschlussgründe (geplant, mit Zählung nach finalem Dedup)
Out of scope (kein Serverless/FaaS‑Benchmarking, z. B. zahlreiche OA‑Fachartikel aus anderen Disziplinen in SP‑CSV) 
Falscher Dokumenttyp (Poster/Slides/Editorials) – wird geprüft
Nicht im Zeitraum (vor 2020) – vereinzelt
Kein frei zugänglicher Volltext (falls keine Preprint‑Version gefunden wird)
Quasi‑Serverless (Cloud Run/Container‑as‑a‑Service) – per Scope ausgeschlossen
Eingeschlossene Studien (kuratiert; OA/Preprint verfügbar; 2020–31.03.2025)
Copik et al. (2021): SeBS: A Serverless Benchmark Suite (Middleware ’21)
Inhalt: Umfassende Benchmark‑Suite mit Micro- und Macro‑Workloads; Metriken: Latenz (warm/cold), Tail (P95/P99), Durchsatz, Kosten; Multi‑Plattform; offene Artefakte. Referenz in vielen Arbeiten 
Qualität: Hoch (Reproduzierbarkeit, Methodik, Plattformbreite)
ICPE’22 (2022): Evaluating the Scalability and Elasticity of Function as a Service Platform
Inhalt: Experimentelle Bewertung von Skalierbarkeit/Elastizität; Design und Messmethoden für Burst/Concurrency; nützlich für Lastprofile/SLOs 
Qualität: Mittel‑hoch (klare Messgrößen, methodisch solide)
ASPLOS’22: FaaSFlow – Efficient workflow execution for FaaS
Inhalt: Workflow‑Optimierungen, End‑to‑End‑Latenzen; relevante Messprotokolle; übertragbar auf Benchmark‑Workloads mit Abhängigkeiten 
Qualität: Hoch
SYSTOR’22: FaaS in the age of (sub‑)μs I/O
Inhalt: I/O‑Pfad und Latenzflanken; liefert Microbenchmark‑Designs für (sub‑)µs‑I/O‑Szenarien 
Qualität: Mittel‑hoch
SoCC’24: Is It Time To Put Cold Starts In The Deep Freeze?
Inhalt: Systematische Behandlung von Cold‑Starts; Evaluations‑Designs und Metriken (Kalt/Warm, Tail) 
Qualität: Hoch
FGCS 2024: Embedding automated function performance benchmarking… in FaaS DevOps pipelines
Inhalt: Automatisierte Benchmarking/Profiling‑Pipelines; Metriken-Extraktion und Kategorisierung 
Qualität: Mittel‑hoch
Computing 2024: Survey on cold start latency approaches in serverless computing
Inhalt: Überblick über Optimierungsansätze; leitet Messgrößen und Kontrollvariablen für Cold‑Start‑Benchmarks ab 
Qualität: Mittel‑hoch
Journal of Cloud Computing 2023: Integrating request replication into FaaS platforms: an experimental evaluation
Inhalt: Experimentelle Evaluation von Request‑Replikation; Effekte auf Latenz/Kosten; reproduzierbar (OA) 
 
Qualität: Mittel‑hoch
Journal of Cloud Computing 2023: Latency and resource consumption analysis for serverless edge analytics
Inhalt: Messungen Latenz/Ressourcenverbräuche im Edge‑Kontext; relevante Micro/Macro‑Benchmarks 
 
Qualität: Mittel
Journal of Cloud Computing 2023: Enriching computing simulators by generating realistic serverless traces
Inhalt: Reale Trace‑Generierung; nützlich zur Workload‑Replikation in Benchmarks 
 
Qualität: Mittel
Journal of Signal Processing Systems 2021: Scheduling Hardware-Accelerated Cloud Functions
Inhalt: Beschleunigte Funktionen, Scheduling‑Einfluss auf Performance; relevante Metriken für GPU/FPGA‑FaaS 
Qualität: Mittel (spezialisierter Fokus)
Journal of Grid Computing 2021: Serverless Workflows for Containerised Applications in the Cloud Continuum
Inhalt: Workflow‑Orchestrierung, Ende‑zu‑Ende‑Latenz und Skalierungsmessungen 
Qualität: Mittel
(Weitere potenzielle Kandidaten aus ACM/IEEE wie XFaaS (SOSP ’23), S-Cache (EdgeSys ’23), SC’24 SMIless sind vorhanden; wir können sie nach OA/Preprint‑Check ergänzen 
 
 
 
)
Qualitätsbewertung (kurz, vorläufig je Studie)
Bewertungsraster: Reproduzierbarkeit (Artefakte offen), interne Validität (Kontrolliertes Design, Replikate), externe Validität (Plattformen/Regionen/Sprachen/Workloads), Metrikgüte (P50/P95/P99, Durchsatz, Kosten), Statistik (CI/Signifikanz), Transparenz (Provider/Region/Runtime/Memory/Concurrency/Timeouts/Features), Aktualität (2020–03/2025, neue Provider‑Features). Oben pro Studie bereits Kurzurteil gegeben; detaillierte Scores trage ich in studies_included.csv ein.
Thematische Kategorisierung (für Synthese)
Benchmark-Typen
Microbenchmarks: Cold/Warm‑Start, Runtime‑Startup, I/O/Netz, CPU/Mem (z. B. Cold‑Start‑Arbeiten, SYSTOR’22) 
Macro-/Applikationsbenchmarks: ML‑Inference, ETL/Data‑Pipelines, Video/Bild, Web‑APIs, Workflows (z. B. SeBS, FaaSFlow, Video‑Pipelines) 
 
Plattform-/Stack-Vergleiche
Public Cloud (AWS/Azure/GCF) und Open‑Source (OpenFaaS/Knative); High‑Concurrency/Autoscaling‑Verhalten 
 
Metriken/SLOs
Latenz (warm/cold), Tail (P95/P99), Durchsatz, Kosteneffizienz, Skalierung/Elastizität, Varianz/Jitter 
 
 
Experimentelles Design
Lastprofile (Burst/Steady), Concurrency/Queueing, Warmup/Keep‑Alive/Provisioned Concurrency, Replikationen/CI 
Systemspezifika
Cold‑Start‑Mitigation (Provisioned Concurrency, SnapStart), Container‑Reuse, Caching/Layers, Beschleuniger (GPU/FPGA) 
 
Was noch offen ist und was ich von dir brauche
Soll ich die PRISMA-Zahlen jetzt wie folgt festziehen?
Records identified: 456 (Springer 425 + ACM 20 + IEEE 11)
Records screened (Title/Abstract): ich setze alle 456 auf „gescreent“, markiere aber die Off‑Topic‑Springer‑Treffer als „Excluded at title“ (Grund: out of scope).
Duplicates removed: ermittele ich exakt per Titel‑Matching aus deinen beiden Springer‑CSV + ACM/IEEE (liefere exakte Zahl im nächsten Step).
Full‑text assessed: alle serverless/FaaS‑relevanten Kandidaten
Included: die oben gelisteten 12 (plus weitere aus ACM/IEEE nach OA‑Check)
Falls du die Google‑Scholar‑Rohzahlen auch formal im PRISMA‑Diagramm ausweisen willst, brauche ich wenigstens eine CSV/PoP‑Export‑Stichprobe (z. B. Top‑200 je Query), sonst bleiben GS‑Treffer in der Dokumentation als „identifiziert, aber nicht importiert/screenbar“ vermerkt.
Bestätige bitte kurz:
Ich soll alle Cloud‑Run/CaaS‑Treffer konsequent ausschließen (Scope).
Thesen/Dissertationen ausschließen (Scope bestätigt).
Sprache der finalen Dokumentation: Deutsch (bestätigt).
Sobald du mir grünes Licht für die PRISMA‑Zählweise gibst oder optional eine kleine GS‑CSV lieferst, sende ich:
ausgefüllte search_log.csv (mit den oben dokumentierten Queries und Treffern),
screening.csv (mit Include/Exclude und Gründen, inkl. exakter Dup‑Zahl),
studies_included.csv (mit Charakterisierung, Kategorien, Qualitätsbewertung).
Quellenhinweise im Text:
SpringerLink‑Exporte: SearchResults (1).csv und SearchResults.csv 
 
ACM DL Suchfeed/Ergebnisse: ergibnis ACMDL.txt 
IEEE Xplore Export (OA/Free-to-Read Zitate): IEEE Xplore Citation Plain Text Download 